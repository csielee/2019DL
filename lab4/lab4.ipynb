{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable():\n",
    "    def __init__(self, data, T=None, grad=None, copy=True):\n",
    "        if data is None or type(data) != np.ndarray:\n",
    "            raise AttributeError('Wrong data type')\n",
    "        \n",
    "        if copy:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            self.data = data\n",
    "        if grad is None:\n",
    "            grad = np.zeros_like(self.data)\n",
    "        self.grad = grad\n",
    "        if T is None:\n",
    "            T = Variable(self.data.T, self, self.grad.T, copy=False)\n",
    "        self.T = T\n",
    "        self.fn = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Variable(\\n{}\\n)\\n'.format(self.data.__str__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.data.__str__()\n",
    "    \n",
    "    def __add__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "            \n",
    "        c = Variable(self.data + b.data)\n",
    "        c.fn = [Variable.__grad_add__, self, b]\n",
    "        #c.fn = c.__wrap_fn(Variable.__grad_add__, self, b)\n",
    "        return c\n",
    "    \n",
    "    def __grad_add__(self, a, b):\n",
    "        a.grad += np.ones_like(a.grad) * self.grad\n",
    "        b.grad += np.ones_like(b.grad) * self.grad\n",
    "    \n",
    "    def __sub__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "        c = Variable(self.data - b.data)\n",
    "        c.fn = [Variable.__grad_sub__, self, b]\n",
    "        #c.fn = c.__wrap_fn(Variable.__grad_sub__, self, b)\n",
    "        return c\n",
    "    \n",
    "    def __grad_sub__(self, a, b):\n",
    "        a.grad += np.ones_like(a.grad) * self.grad\n",
    "        b.grad -= np.ones_like(b.grad) * self.grad\n",
    "    \n",
    "    def __mul__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "        \n",
    "        c = Variable(self.data * b.data)\n",
    "        c.fn = [Variable.__grad_mul__, self, b]\n",
    "        #c.fn = c.__wrap_fn(Variable.__grad_mul__, self, b)\n",
    "        return c\n",
    "    \n",
    "    def __grad_mul__(self, a, b):\n",
    "        a.grad += b.data * self.grad\n",
    "        b.grad += a.data * self.grad\n",
    "    \n",
    "    def __matmul__(self, b):\n",
    "        c = Variable(np.matmul(self.data, b.data))\n",
    "        c.fn = [Variable.__grad_matmul__, self, b]\n",
    "        #c.fn = c.__wrap_fn(Variable.__grad_matmul__, self, b)\n",
    "        return c\n",
    "    \n",
    "    def __grad_matmul__(self, a, b):\n",
    "        a.grad += np.matmul(self.grad, b.data.T)\n",
    "        b.grad += np.matmul(a.data.T, self.grad)\n",
    "    \n",
    "    \n",
    "    def tanh(self):\n",
    "        c = Variable(np.tanh(self.data))\n",
    "        c.fn = [Variable.__grad_tanh__, self]\n",
    "        return c\n",
    "        \n",
    "    def __grad_tanh__(self, a):\n",
    "        a.grad += self.grad * (1 - (self.data**2))\n",
    "    \n",
    "    def crossentropy(self, target):\n",
    "        s = self.__softmax(1)\n",
    "        if type(target) is Variable:\n",
    "            target = target.data\n",
    "            \n",
    "        target = target.astype(np.int)\n",
    "        \n",
    "        if target.shape[0] > 1:\n",
    "            slis = tuple(zip(range(target.shape[0]), target))\n",
    "        else:\n",
    "            slis = (0, target[0])\n",
    "        \n",
    "        c = Variable(np.array(np.sum(-np.log(s[slis]))))\n",
    "        c.fn = [Variable.__grad_corssentropy, self, target]\n",
    "        return c\n",
    "    \n",
    "    def __grad_corssentropy(self, a, target):\n",
    "        y = np.zeros_like(a.grad)\n",
    "        if target.shape[0] > 1:\n",
    "            slis = tuple(zip(range(target.shape[0]), target))\n",
    "        else:\n",
    "            slis = (0, target[0])\n",
    "            \n",
    "        y[slis] = 1.0\n",
    "        a.grad += (a.__softmax(1) - y)\n",
    "    \n",
    "    def __softmax(self, dim):\n",
    "        # move dim idxs\n",
    "        exp_data = np.exp(self.data)\n",
    "        return exp_data / np.sum(exp_data, axis=dim).reshape([-1]+[1 for _ in range(dim)])\n",
    "    \n",
    "    def backward(self, backward_grad):\n",
    "        if type(backward_grad) is Variable:\n",
    "            backward_grad = backward_grad.data\n",
    "        \n",
    "        if backward_grad.shape != self.data.shape:\n",
    "            raise AttributeError('Wrong backward grad shape {} != {}'.format(backward_grad.shape, self.data.shape))\n",
    "        \n",
    "        self.grad = backward_grad\n",
    "        self.__backward()\n",
    "        \n",
    "    # use grad as nparray\n",
    "    def __backward(self):\n",
    "        if self.fn is None:\n",
    "            return;\n",
    "        \n",
    "        backward_op = self.fn[0]\n",
    "        \n",
    "        backward_op(self, *self.fn[1:])\n",
    "        \n",
    "        for v in self.fn[1:]:\n",
    "            if type(v) is Variable:\n",
    "                v.__backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.284946979038115 tensor(2.2849, dtype=torch.float64, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def testGrad():\n",
    "    n = 1\n",
    "    c = 5\n",
    "    x = np.random.random((n,2))\n",
    "    h = np.random.random((n,16))\n",
    "    y = np.random.randint(0, c, n)\n",
    "    \n",
    "    u = np.random.random((2, 16))\n",
    "    w = np.random.random((16,16))\n",
    "    v = np.random.random((16, c))\n",
    "    \n",
    "    def equation(*args):\n",
    "        x, h, u, w, v = args\n",
    "        return ((x @ u) + (h @ w)).tanh() @ v\n",
    "    \n",
    "    def allv(l):\n",
    "        return [Variable(i) for i in l]\n",
    "\n",
    "    def allt(l):\n",
    "        return [torch.tensor(i, requires_grad=True) for i in l]\n",
    "    \n",
    "    wow = [x, h, u, w, v]\n",
    "    \n",
    "    Vs = allv(wow)\n",
    "    Ts = allt(wow)\n",
    "    Y = Variable(y)\n",
    "    _Y = torch.tensor(y.astype(np.float), requires_grad=True)\n",
    "    \n",
    "    A = equation(*Vs)\n",
    "    A = A.crossentropy(Y)\n",
    "    _A = equation(*Ts)\n",
    "    _A = torch.nn.CrossEntropyLoss(reduction='sum')(_A, _Y.long())\n",
    "    \n",
    "    print(A, _A)\n",
    "    \n",
    "    A.backward(np.array(1))\n",
    "    _A.backward()\n",
    "    \n",
    "    t = None\n",
    "    \n",
    "    for V, T in zip(Vs, Ts):\n",
    "        r = (torch.tensor(V.data) == T).all()\n",
    "        if t is None:\n",
    "            t = r\n",
    "        else:\n",
    "            t &= r\n",
    "    \n",
    "    return t\n",
    "testGrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
