{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_mean = 0.1307\n",
    "MNIST_std = 0.3081\n",
    "\n",
    "MNIST_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        (MNIST_mean,), (MNIST_std,)\n",
    "    )\n",
    "])\n",
    "\n",
    "MNIST_inverse_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Lambda(lambda x: x*MNIST_std + MNIST_mean),\n",
    "    torchvision.transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(\n",
    "    './mnist_datset', train=True, download=True, \n",
    "    transform=MNIST_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MNIST_inverse_transform(mnist_dataset[5000][0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(noise_size, 512, 4, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 2, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 1, 1, 1, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "        self.noise_size = noise_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "    \n",
    "class FrontEnd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrontEnd, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(1, 64, 1, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 2, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, 4, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        #return self.main(x).view(-1, 1)\n",
    "        return x.view(-1,1)\n",
    "    \n",
    "class Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q, self).__init__()\n",
    "        \n",
    "        #self.main = nn.Sequential(\n",
    "        #    nn.Conv2d(1024, 128, 1, bias=False),\n",
    "        #    nn.ReLU(True),\n",
    "        #    nn.Conv2d(128, 10, 1)\n",
    "        #)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(8192, 100, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.main(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train InfoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, generator, front_end, discriminator, Q, metrics=[]):\n",
    "        self.generator = generator.cuda()\n",
    "        self.front_end = front_end.cuda()\n",
    "        self.discriminator = discriminator.cuda()\n",
    "        self.Q = Q.cuda()\n",
    "        \n",
    "        self.noise_size = generator.noise_size\n",
    "        self.batch_size = 100\n",
    "    \n",
    "        self.metrics = metrics\n",
    "        self.seed = 123\n",
    "    \n",
    "    def refresh_metrics(self):\n",
    "        tmp = self.metrics\n",
    "        self.metrics = []\n",
    "        return tmp\n",
    "    \n",
    "    def _noise_eval(self):\n",
    "        cluster_num = 10\n",
    "        \n",
    "        # gen all cluster one hot vectors\n",
    "        idx = np.arange(cluster_num)\n",
    "        c = np.zeros((cluster_num, cluster_num))\n",
    "        c[range(cluster_num), idx] = 1.0\n",
    "        \n",
    "        # gen torch (#cluster, #noise, 1, 1) with same noise\n",
    "        z = torch.cat([\n",
    "            torch.FloatTensor(1, self.noise_size - cluster_num).uniform_(-1.0, 1.0).expand(cluster_num, self.noise_size - cluster_num), \n",
    "            torch.Tensor(c)\n",
    "        ] , 1).view(-1, self.noise_size, 1, 1)\n",
    "        \n",
    "        return z.cuda(), torch.LongTensor(idx).cuda()\n",
    "    \n",
    "    def _noise_idx(self, idx):\n",
    "        cluster_num = 10\n",
    "        \n",
    "        c = np.zeros((len(idx), cluster_num))\n",
    "        c[range(len(idx)), idx] = 1.0\n",
    "        \n",
    "        z = torch.cat([\n",
    "            torch.FloatTensor(len(idx), self.noise_size - cluster_num).uniform_(-1.0, 1.0),\n",
    "            torch.Tensor(c)\n",
    "        ], 1).view(-1, self.noise_size, 1, 1)\n",
    "        \n",
    "        return z.cuda(), torch.LongTensor(idx).cuda()\n",
    "        \n",
    "    def _noise_sample(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        # gen condition c\n",
    "        cluster_num = 10\n",
    "        idx = np.random.randint(cluster_num, size=batch_size)\n",
    "        c = np.zeros((batch_size, cluster_num))\n",
    "        c[range(batch_size), idx] = 1.0\n",
    "        \n",
    "        # gen torch (batch, #noise, 1, 1)\n",
    "        z = torch.cat([\n",
    "            torch.FloatTensor(batch_size, self.noise_size - cluster_num).uniform_(-1.0, 1.0), \n",
    "            torch.Tensor(c)\n",
    "        ] , 1).view(-1, self.noise_size, 1, 1)\n",
    "        \n",
    "        return z.cuda(), torch.LongTensor(idx).cuda()\n",
    "    \n",
    "    def train(self, dataset, batch_size, epoch_size, show=False):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        criterion_D = nn.BCELoss().cuda()\n",
    "        criterion_Q = nn.CrossEntropyLoss().cuda()\n",
    "        \n",
    "        optim_D = optim.Adam(\n",
    "            [{'params':self.front_end.parameters()}, {'params':self.discriminator.parameters()}],\n",
    "            lr = 0.0002, betas=(0.5, 0.99)\n",
    "        )\n",
    "        optim_G = optim.Adam(\n",
    "            [{'params':self.generator.parameters()}, {'params':self.Q.parameters()}],\n",
    "            lr = 0.001, betas=(0.5, 0.99)\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "        \n",
    "        # check train mode\n",
    "        self.generator.train()\n",
    "        self.front_end.train()\n",
    "        self.discriminator.train()\n",
    "        self.Q.train()\n",
    "        \n",
    "        # prepare some tensor for training\n",
    "        label = torch.FloatTensor(self.batch_size, 1).cuda()\n",
    "        \n",
    "        for epoch in range(len(self.metrics), epoch_size):\n",
    "            batch_metrics = []\n",
    "            for batch, batch_data in enumerate(dataloader, 0):\n",
    "                \n",
    "                # Discriminator part\n",
    "                optim_D.zero_grad()\n",
    "                \n",
    "                # Feed real image\n",
    "                x, _ = batch_data\n",
    "                \n",
    "                bs = x.size(0)\n",
    "                label.data.resize_(bs, 1)\n",
    "                \n",
    "                real_x = x.cuda()\n",
    "                \n",
    "                real_out = self.discriminator(self.front_end(real_x))\n",
    "                \n",
    "                label.data.fill_(1.0)\n",
    "                loss_D_real = criterion_D(real_out, label)\n",
    "                \n",
    "                loss_D_real.backward()\n",
    "                \n",
    "                # Feed fake image from generator\n",
    "                z, idx = self._noise_sample(bs)\n",
    "                \n",
    "                fake_x = self.generator(z)\n",
    "                fake_out = self.discriminator(self.front_end(fake_x.detach()))\n",
    "                \n",
    "                label.data.fill_(0.0)\n",
    "                loss_D_fake = criterion_D(fake_out, label)\n",
    "                \n",
    "                loss_D_fake.backward()\n",
    "                \n",
    "                optim_D.step()\n",
    "                \n",
    "                loss_D = loss_D_real + loss_D_fake\n",
    "                \n",
    "                # Generator and Q part\n",
    "                optim_G.zero_grad()\n",
    "                \n",
    "                fe_out = self.front_end(fake_x)\n",
    "                ad_out = self.discriminator(fe_out)\n",
    "                \n",
    "                label.data.fill_(1.0)\n",
    "                \n",
    "                loss_G_reconstruct = criterion_D(ad_out, label)\n",
    "                \n",
    "                c = self.Q(fe_out)\n",
    "\n",
    "                loss_Q = criterion_Q(c, idx)\n",
    "                \n",
    "                loss_G = loss_G_reconstruct + loss_Q\n",
    "                loss_G.backward()\n",
    "                \n",
    "                optim_G.step()\n",
    "                \n",
    "                fake_x = self.generator(z)\n",
    "                fake_out_after = self.discriminator(self.front_end(fake_x.detach()))\n",
    "                \n",
    "                if batch % 100 == 0:\n",
    "                    # show logs\n",
    "                    print('Epoch/Batch:{:4d}/{:4d}, loss G: {:4.2f}. loss D: {:4.2f} loss Q: {:4.2f}'.format(\n",
    "                        epoch, batch, loss_G.item(), loss_D.item(), loss_Q.item()\n",
    "                    ))\n",
    "                    if show:\n",
    "                        showImage(self.evaluation(seed=self.seed))\n",
    "                    clear_output(wait=True)\n",
    "                \n",
    "                # calculate probability\n",
    "                # real\n",
    "                r = torch.mean(real_out).item()\n",
    "                # fake before\n",
    "                f_b = torch.mean(fake_out).item()\n",
    "                # fake after\n",
    "                f_a = torch.mean(fake_out_after).item()\n",
    "                \n",
    "                batch_metrics.append( (loss_G.item(), loss_D.item(), loss_Q.item(), r, f_b, f_a) )\n",
    "                    \n",
    "            # store metrics\n",
    "            self.metrics.append( batch_metrics )\n",
    "    \n",
    "    def evaluation(self, noise_num=10, seed=None):\n",
    "        \n",
    "        if seed is None:\n",
    "            import time\n",
    "            seed = time.time()\n",
    "        \n",
    "        self.generator.eval()\n",
    "        \n",
    "        x = []\n",
    "        torch.manual_seed(seed)\n",
    "        for i in range(noise_num):\n",
    "            z, _ = self._noise_eval()\n",
    "            x.append( self.generator(z) )\n",
    "        \n",
    "        x = torch.cat(x, dim=0)\n",
    "        \n",
    "        self.generator.train()\n",
    "        \n",
    "        return x.detach().cpu()\n",
    "    \n",
    "    def evaluation_with_idx(self, idxs, seed=None):\n",
    "        \n",
    "        if seed is None:\n",
    "            import time\n",
    "            seed = time.time()\n",
    "        \n",
    "        self.generator.eval()\n",
    "        \n",
    "        x = []\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        z, _ = self._noise_idx(idxs)\n",
    "        \n",
    "        x = self.generator(z)\n",
    "        \n",
    "        self.generator.train()\n",
    "        \n",
    "        return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataToImage(data, row, col):\n",
    "    return MNIST_inverse_transform(\n",
    "        torch.cat([\n",
    "            torch.cat([\n",
    "                data[i*col + j] \n",
    "                for j in range(col)\n",
    "            ] , dim=2)\n",
    "            for i in range(row)\n",
    "        ], dim=1)\n",
    "    )\n",
    "\n",
    "def showImage(data):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.imshow(convertDataToImage(data, 10, 10), cmap='gray')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def saveImage(fig, epoch, root):\n",
    "    if not os.path.isdir(root):\n",
    "        os.mkdir(v1)\n",
    "    fn = os.path.join(root, 'result-{}.png'.format(epoch))\n",
    "    fig.savefig(\n",
    "        fn, dpi=300,bbox_inches='tight',\n",
    "        metadata = {\n",
    "            'author' : 'TonyLee'\n",
    "        }\n",
    "    )\n",
    "\n",
    "def demo(trainer, idxs):\n",
    "    plt.figure(figsize=(3, 3*len(idxs)))\n",
    "    plt.imshow(convertDataToImage(trainer.evaluation_with_idx(idxs), len(idxs), 1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showCurve(metrics, window_size=100):\n",
    "    from functools import reduce\n",
    "    a = np.array(reduce(lambda x,y : x+y, metrics))\n",
    "    a = np.concatenate(\n",
    "    #[\n",
    "    #   a[0:1, :].reshape(1, -1)\n",
    "    #] +\n",
    "    [\n",
    "        np.average(a[i*window_size:(i+1)*window_size, :], axis=0).reshape(1, -1) \n",
    "        for i in range(a.shape[0] // window_size)\n",
    "    ] , axis=0)\n",
    "    \n",
    "    s = (12,3)\n",
    "    \n",
    "    x = range(1, a.shape[0]+1)\n",
    "    \n",
    "    # show loss curve\n",
    "    plt.figure(figsize=s)\n",
    "    plt.title('Loss curve\\nGenerator, Discriminator, Q')\n",
    "    plt.plot(x,a[:,0], label='loss G')\n",
    "    plt.plot(x,a[:,1], label='loss D')\n",
    "    plt.plot(x,a[:,2], label='loss Q')\n",
    "    plt.xlabel('each {} batch'.format(window_size))\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # show probability curve\n",
    "    plt.figure(figsize=s)\n",
    "    plt.title('Probability curve')\n",
    "    plt.plot(x,a[:,3], label='P(real data)')\n",
    "    plt.plot(x,a[:,4], label='P(fake before update)')\n",
    "    plt.plot(x,a[:,5], label='P(fake after  update)')\n",
    "    plt.xlabel('each {} batch'.format(window_size))\n",
    "    plt.ylabel('probability')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trainer(trainer, root):\n",
    "    if not os.path.isdir(root):\n",
    "        os.mkdir(root)\n",
    "    p = os.path.join(root, 'trainer.pkl')\n",
    "    torch.save(trainer, p)\n",
    "    return p\n",
    "    \n",
    "def load_trainer(root):\n",
    "    p = os.path.join(root, 'trainer.pkl')\n",
    "    return torch.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(756110)\n",
    "#torch.manual_seed(850204850226.5201314)\n",
    "t = Trainer(Generator(64), FrontEnd(), Discriminator(), Q(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t.train(mnist_dataset, 64, 1000, show=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showCurve(t.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = showImage(t.evaluation(seed=123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Trainer(t.generator, t.front_end, t.discriminator, t.Q, t.metrics)\n",
    "save_trainer(tmp, './v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = load_trainer('./v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t2.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
