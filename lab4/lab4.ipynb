{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable():\n",
    "    def __init__(self, data, T=None, grad=None, copy=True):\n",
    "        if data is None or type(data) != np.ndarray:\n",
    "            raise AttributeError('Wrong data type')\n",
    "        \n",
    "        if copy:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            self.data = data\n",
    "        if grad is None:\n",
    "            grad = np.zeros_like(self.data)\n",
    "        self.grad = grad\n",
    "        if T is None:\n",
    "            T = Variable(self.data.T, self, self.grad.T, copy=False)\n",
    "        self.T = T\n",
    "        self.fn = None\n",
    "        self.child = []\n",
    "        self.ready = False\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad[:,:] = 0.0\n",
    "        self.child = []\n",
    "        self.ready = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Variable(\\n{}\\n)\\n'.format(self.data.__str__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.data.__str__()\n",
    "    \n",
    "    def __add__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "            \n",
    "        c = Variable(self.data + b.data)\n",
    "        c.fn = [Variable.__grad_add__, self, b]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_add__(self, a, b):\n",
    "        a.grad += np.ones_like(a.grad) * self.grad\n",
    "        b.grad += np.ones_like(b.grad) * self.grad\n",
    "    \n",
    "    def __sub__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "        c = Variable(self.data - b.data)\n",
    "        c.fn = [Variable.__grad_sub__, self, b]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_sub__(self, a, b):\n",
    "        a.grad += np.ones_like(a.grad) * self.grad\n",
    "        b.grad -= np.ones_like(b.grad) * self.grad\n",
    "    \n",
    "    def __mul__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "        \n",
    "        c = Variable(self.data * b.data)\n",
    "        c.fn = [Variable.__grad_mul__, self, b]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_mul__(self, a, b):\n",
    "        a.grad += b.data * self.grad\n",
    "        b.grad += a.data * self.grad\n",
    "    \n",
    "    def __matmul__(self, b):\n",
    "        c = Variable(np.matmul(self.data, b.data))\n",
    "        c.fn = [Variable.__grad_matmul__, self, b]\n",
    "           \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_matmul__(self, a, b):\n",
    "        a.grad += np.matmul(self.grad, b.data.T)\n",
    "        b.grad += np.matmul(a.data.T, self.grad)\n",
    "    \n",
    "    \n",
    "    def tanh(self):\n",
    "        c = Variable(np.tanh(self.data))\n",
    "        c.fn = [Variable.__grad_tanh__, self]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        return c\n",
    "        \n",
    "    def __grad_tanh__(self, a):\n",
    "        a.grad += self.grad * (1 - (self.data**2))\n",
    "    \n",
    "    def crossentropy(self, target):\n",
    "        s = self.softmax(1)\n",
    "        if type(target) is Variable:\n",
    "            target = target.data\n",
    "            \n",
    "        target = target.astype(np.int)\n",
    "        \n",
    "        if target.shape[0] > 1:\n",
    "            slis = tuple(zip(range(target.shape[0]), target))\n",
    "        else:\n",
    "            slis = (0, target[0])\n",
    "        \n",
    "        c = Variable(np.array(np.sum(-np.log(s[slis]))))\n",
    "        c.fn = [Variable.__grad_corssentropy, self, target]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_corssentropy(self, a, target):\n",
    "        y = np.zeros_like(a.grad)\n",
    "        if target.shape[0] > 1:\n",
    "            slis = tuple(zip(range(target.shape[0]), target))\n",
    "        else:\n",
    "            slis = (0, target[0])\n",
    "            \n",
    "        y[slis] = 1.0\n",
    "        a.grad += (a.softmax(1) - y)\n",
    "    \n",
    "    def softmax(self, dim):\n",
    "        # move dim idxs\n",
    "        exp_data = np.exp(self.data)\n",
    "        return exp_data / np.sum(exp_data, axis=dim).reshape([-1]+[1 for _ in range(dim)])\n",
    "    \n",
    "    def argsoftmax(self):\n",
    "        s = self.softmax(1)\n",
    "        return Variable(np.argmax(s, axis=1).reshape(-1,1))\n",
    "    \n",
    "    def backward(self, backward_grad):\n",
    "        if type(backward_grad) is Variable:\n",
    "            backward_grad = backward_grad.data\n",
    "        \n",
    "        if backward_grad.shape != self.data.shape:\n",
    "            raise AttributeError('Wrong backward grad shape {} != {}'.format(backward_grad.shape, self.data.shape))\n",
    "        \n",
    "        self.grad = backward_grad\n",
    "        self.__backward()\n",
    "    \n",
    "    def __backward(self):\n",
    "        if self.fn is None:\n",
    "            return;\n",
    "        \n",
    "        # check self grad is ready, trace child variables\n",
    "        self.ready = True\n",
    "        for child in self.child:\n",
    "            self.ready &= child.ready\n",
    "        \n",
    "        if not self.ready:\n",
    "            return;\n",
    "        \n",
    "        backward_op = self.fn[0]\n",
    "        \n",
    "        backward_op(self, *self.fn[1:])\n",
    "        \n",
    "        for v in self.fn[1:]:\n",
    "            if type(v) is Variable:\n",
    "                v.__backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels):\n",
    "        self.U = Variable(np.random.uniform(-1,1, (in_channels, hidden_channels)))\n",
    "        self.W = Variable(np.random.uniform(-1,1, (hidden_channels, hidden_channels)))\n",
    "        self.V = Variable(np.random.uniform(-1,1, (hidden_channels, out_channels)))\n",
    "        self.b = Variable(np.random.uniform(-1,1, (1, hidden_channels)))\n",
    "        self.c = Variable(np.random.uniform(-1,1, (1, out_channels)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        t = len(x)\n",
    "        self.h = None\n",
    "        y = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            a = self.b + (x[i] @ self.U)\n",
    "            if self.h is not None:\n",
    "                a += (self.h @ self.W)\n",
    "        \n",
    "            self.h = a.tanh()\n",
    "            \n",
    "            o = self.c + (self.h @ self.V)\n",
    "            y.append(o)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.U.zero_grad()\n",
    "        self.W.zero_grad()\n",
    "        self.V.zero_grad()\n",
    "        self.b.zero_grad()\n",
    "        self.c.zero_grad()\n",
    "    \n",
    "    def step(self, lr=1e-2):\n",
    "        if lr is None:\n",
    "            lr = 1e-2\n",
    "        self.U.data -= lr * self.U.grad\n",
    "        self.W.data -= lr * self.W.grad\n",
    "        self.V.data -= lr * self.V.grad\n",
    "        self.b.data -= lr * self.b.grad\n",
    "        self.c.data -= lr * self.c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toBinaray(x, digits, complement=False):\n",
    "    if complement and x < 0:\n",
    "        x += (1<<(digits))\n",
    "    x = abs(x)\n",
    "    return [ float(int(i)) for i in list((\"{:0\" + str(digits) + \"b}\").format(x))[::-1]][:digits]\n",
    "\n",
    "def toNumber(b, complement=False):\n",
    "    if complement:\n",
    "        last = b[-1]\n",
    "        b = b[:-1]\n",
    "    d = sum([int(x)<<i for i, x in enumerate(b)])\n",
    "    if complement:\n",
    "        d -= int(last)*(1<<(len(b)))\n",
    "    return d\n",
    "\n",
    "def BinaryDataset(digits=8):\n",
    "    thr = (1<<(digits-1))\n",
    "    while True:\n",
    "        a, b = np.random.randint(0, thr, 2)\n",
    "        c = a + b\n",
    "        x = np.array([toBinaray(a, digits), toBinaray(b, digits)])\n",
    "        y = np.array([toBinaray(c, digits)])\n",
    "        yield [Variable(x.T[i:i+1, :]) for i in range(digits)], [Variable(y.T[i:i+1, :]) for i in range(digits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, digits, epoch_size, lr=None, show_size=1000):\n",
    "    dataset = BinaryDataset(digits)\n",
    "    \n",
    "    all_error = []\n",
    "    all_loss = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    for epoch in range(epoch_size):\n",
    "        x, y = next(dataset)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        loss = [output[i].crossentropy(y[i]) for i in range(len(y))]\n",
    "        \n",
    "        for l in loss[::-1]:\n",
    "            l.backward(np.array(1))\n",
    "            \n",
    "        model.step(lr)\n",
    "        \n",
    "        e = np.count_nonzero([np.all(output[i].argsoftmax().data != y[i].data) for i in range(len(y))])\n",
    "        all_error.append(e)\n",
    "        all_loss.append(sum([l.data for l in loss]))\n",
    "        \n",
    "        if e == 0:\n",
    "            all_accuracy.append(1)\n",
    "        else:\n",
    "            all_accuracy.append(0)\n",
    "        \n",
    "        if (epoch+1) % show_size == 0:\n",
    "            print('[{:5d}] error : {}, loss : {}'.format(epoch+1, sum(all_error[-show_size:])/show_size, sum(all_loss[-show_size:])/show_size ))\n",
    "        \n",
    "    return all_error, all_loss, all_accuracy\n",
    "\n",
    "def __show(all_error, all_loss, all_accuracy, mean_size=100):\n",
    "    x = [i*mean_size for i in range(len(all_error)//mean_size)]\n",
    "    y = [sum(all_error[i*mean_size:(i+1)*mean_size])/ mean_size for i in range(len(all_error)//mean_size)]\n",
    "    y2 = [sum(all_loss[i*mean_size:(i+1)*mean_size])/ mean_size for i in range(len(all_loss)//mean_size)]\n",
    "    y3 = [sum(all_accuracy[i*mean_size:(i+1)*mean_size])/ mean_size for i in range(len(all_accuracy)//mean_size)]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('Episode Accuracy')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Error / Loss')\n",
    "    plt.plot(x, y, '-o', label='error')\n",
    "    plt.plot(x, y2, '-o', label='loss')\n",
    "    plt.plot(x, y3, '-o', label='accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def show(all_accuracy, mean_size=100):\n",
    "    x = [((i+0.5)*mean_size) for i in range(len(all_accuracy)//mean_size)]\n",
    "    y = [sum(all_accuracy[i*mean_size:(i+1)*mean_size])/ mean_size for i in range(len(all_accuracy)//mean_size)]\n",
    "    #y = np.convolve(np.array(all_accuracy), np.ones(mean_size)/mean_size, mode='same')\n",
    "    '''\n",
    "    y = []\n",
    "    for i in range(len(all_accuracy)):\n",
    "        p = i - mean_size\n",
    "        if p < 0:\n",
    "            p = 0\n",
    "        y.append(sum(all_accuracy[p : i+1]) / (i+1 - p))\n",
    "        \n",
    "    x = list(range(len(y)))\n",
    "    '''\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('Episode Accuracy')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(x, y, '-o', label='accuracy')\n",
    "    \n",
    "    plt.xlim((0, len(all_accuracy)))\n",
    "    plt.xticks(x)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def evaluation(model, digits, epoch_size, show_size):\n",
    "    accuracy = 0\n",
    "    \n",
    "    all_error = []\n",
    "    all_accuracy = []\n",
    "\n",
    "    eval_dataset = BinaryDataset(digits)\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        x, y = next(eval_dataset)\n",
    "    \n",
    "        output = model.forward(x)\n",
    "    \n",
    "        e = np.count_nonzero([np.all(output[i].argsoftmax().data != y[i].data) for i in range(len(y))])\n",
    "        all_error.append(e)\n",
    "        \n",
    "        if e == 0:\n",
    "            all_accuracy.append(1)\n",
    "        else:\n",
    "            all_accuracy.append(0)\n",
    "    \n",
    "        output = [float(o.argsoftmax().data) for o in output]\\\n",
    "    \n",
    "        x = np.concatenate([v.data for v in x]).T\n",
    "        y = np.concatenate([v.data for v in y]).T\n",
    "        \n",
    "        if (epoch+1) % show_size == 0:\n",
    "            print('[{:5d}] error : {}\\t{:d} + {:d} = {:d}, model:{:d}'.format(\n",
    "                epoch+1, sum(all_error[-show_size:])/show_size,\n",
    "                toNumber(x[0,:]), toNumber(x[1, :]), toNumber(y[0,:]), toNumber(output)\n",
    "            ))\n",
    "    \n",
    "    accuracy =  sum(all_accuracy) / epoch_size\n",
    "    if show_size < epoch_size:\n",
    "        print('Accuracy : {:.3f}%'.format(accuracy))\n",
    "    \n",
    "    return accuracy, all_error, all_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1000] error : 3.341, loss : 5.468539686055281\n",
      "[ 2000] error : 0.955, loss : 2.2620403270024196\n",
      "[ 3000] error : 0.0, loss : 0.09737654050254296\n",
      "[ 4000] error : 0.0, loss : 0.042067430591132264\n",
      "[ 5000] error : 0.0, loss : 0.026662609860304696\n",
      "[ 6000] error : 0.0, loss : 0.019337575534873683\n",
      "[ 7000] error : 0.0, loss : 0.015071218592531514\n",
      "[ 8000] error : 0.0, loss : 0.012315076144171808\n",
      "[ 9000] error : 0.0, loss : 0.010412036248920994\n",
      "[10000] error : 0.0, loss : 0.00904541116671281\n"
     ]
    }
   ],
   "source": [
    "model = RNN(2, 2, 16)\n",
    "e, l, a = train(model, 8, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(a, mean_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  100] error : 0.0\t284180217812168 + 427418231364109 = 711598449176277, model:711598449176277\n",
      "[  200] error : 0.0\t106086698351916 + 381099900944865 = 487186599296781, model:487186599296781\n",
      "[  300] error : 0.0\t493609432284964 + 429408245583437 = 923017677868401, model:923017677868401\n",
      "[  400] error : 0.0\t201388324105683 + 193727802732695 = 395116126838378, model:395116126838378\n",
      "[  500] error : 0.0\t555837788854974 + 105086370267621 = 660924159122595, model:660924159122595\n",
      "[  600] error : 0.0\t541954122925236 + 131979578890126 = 673933701815362, model:673933701815362\n",
      "[  700] error : 0.0\t314048864387329 + 483572529896816 = 797621394284145, model:797621394284145\n",
      "[  800] error : 0.0\t111507964639023 + 367567337132449 = 479075301771472, model:479075301771472\n",
      "[  900] error : 0.0\t138062906325698 + 518298847721268 = 656361754046966, model:656361754046966\n",
      "[ 1000] error : 0.0\t384076531568720 + 369948375531635 = 754024907100355, model:754024907100355\n",
      "Accuracy : 1.000%\n"
     ]
    }
   ],
   "source": [
    "a = evaluation(model, 50, 1000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accs = []\n",
    "for td in [2, 4, 6, 8]:\n",
    "    accs = []\n",
    "    __model = RNN(2, 2, 16)\n",
    "    train(__model, td, 10000, show_size=20000)\n",
    "    for ed in range(10, 51, 5):\n",
    "        acc, _, _ = evaluation(__model, ed, 1000, show_size=1001)\n",
    "        accs.append(acc)\n",
    "    print('Done ', td)\n",
    "    all_accs.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.title('Extend number of binary bit')\n",
    "for i, td in enumerate([2, 4, 6, 8]):\n",
    "    x = list(range(10, 51, 5))\n",
    "    y = all_accs[i]\n",
    "    plt.plot(x, y, label='train digits = '+str(td))\n",
    "plt.legend()\n",
    "plt.xlabel('evaluation digits')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accs = []\n",
    "for hu in range(1,16):\n",
    "    accs = []\n",
    "    __model = RNN(2, 2, hu)\n",
    "    train(__model, 8, 10000, show_size=20000)\n",
    "    for ed in range(8, 49, 8):\n",
    "        acc, _, _ = evaluation(__model, ed, 1000, show_size=1001)\n",
    "        accs.append(acc)\n",
    "    print('Done ', hu)\n",
    "    all_accs.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.title('Diff hidden unit in model')\n",
    "x = list(range(1,16))\n",
    "y = []\n",
    "for i, td in enumerate(x):\n",
    "    y.append(sum(all_accs[i]) / len(all_accs[i])) \n",
    "\n",
    "plt.plot(x, y)\n",
    "#plt.legend()\n",
    "plt.xlabel('number of hidden unit')\n",
    "plt.ylabel('average of accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorchRNN():\n",
    "    def __init__(self, model):\n",
    "        self.U = torch.tensor(model.U.data, requires_grad=True)\n",
    "        self.W = torch.tensor(model.W.data, requires_grad=True)\n",
    "        self.V = torch.tensor(model.V.data, requires_grad=True)\n",
    "        self.b = torch.tensor(model.b.data, requires_grad=True)\n",
    "        self.c = torch.tensor(model.c.data, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        t = len(x)\n",
    "        self.h = None\n",
    "        y = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            a = self.b + (torch.tensor(x[i].data, requires_grad=True) @ self.U)\n",
    "            if self.h is not None:\n",
    "                a += (self.h @ self.W)\n",
    "            self.h = a.tanh()\n",
    "            \n",
    "            o = self.c + (self.h @ self.V)\n",
    "            y.append(o)\n",
    "            \n",
    "        return y\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        if self.U.grad is None:\n",
    "            return;\n",
    "        with torch.no_grad():\n",
    "            self.U.grad.zero_()\n",
    "            if self.W.grad is not None:\n",
    "                self.W.grad.zero_()\n",
    "            self.V.grad.zero_()\n",
    "            self.b.grad.zero_()\n",
    "            self.c.grad.zero_()\n",
    "    \n",
    "    def step(self, lr=1e-1):\n",
    "        if self.U.grad is None:\n",
    "            return;\n",
    "        with torch.no_grad():\n",
    "            self.U -= lr * self.U.grad\n",
    "            if self.W.grad is not None:\n",
    "                self.W -= lr * self.W.grad\n",
    "            self.V -= lr * self.V.grad\n",
    "            self.b -= lr * self.b.grad\n",
    "            self.c -= lr * self.c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def testGrad():\n",
    "    n = 1\n",
    "    c = 5\n",
    "    x = np.random.random((n,2))\n",
    "    h = np.random.random((n,16))\n",
    "    y = np.random.randint(0, c, n)\n",
    "    \n",
    "    u = np.random.random((2, 16))\n",
    "    w = np.random.random((16,16))\n",
    "    v = np.random.random((16, c))\n",
    "    \n",
    "    def equation(*args):\n",
    "        x, h, u, w, v = args\n",
    "        h1 = ((x @ u) + (h @ w)).tanh()\n",
    "        return ((x @ u) + (h1 @ w)).tanh() @ v\n",
    "    \n",
    "    def allv(l):\n",
    "        return [Variable(i) for i in l]\n",
    "\n",
    "    def allt(l):\n",
    "        return [torch.tensor(i, requires_grad=True) for i in l]\n",
    "    \n",
    "    wow = [x, h, u, w, v]\n",
    "    \n",
    "    Vs = allv(wow)\n",
    "    Ts = allt(wow)\n",
    "    Y = Variable(y)\n",
    "    _Y = torch.tensor(y.astype(np.float), requires_grad=True)\n",
    "    \n",
    "    A = equation(*Vs)\n",
    "    A = A.crossentropy(Y)\n",
    "    _A = equation(*Ts)\n",
    "    _A = torch.nn.CrossEntropyLoss(reduction='sum')(_A, _Y.long())\n",
    "    #_A = torch.nn.MSELoss()(_A, _A)\n",
    "    \n",
    "    print(A, _A)\n",
    "    \n",
    "    A.backward(np.array(1))\n",
    "    _A.backward()\n",
    "    \n",
    "    t = None\n",
    "    \n",
    "    for V, T in zip(Vs, Ts):\n",
    "        r = torch.allclose(torch.tensor(V.grad), T.grad)\n",
    "        print(torch.allclose(torch.tensor(V.grad), T.grad))\n",
    "        if t is None:\n",
    "            t = r\n",
    "        else:\n",
    "            t &= r\n",
    "    \n",
    "    return t\n",
    "testGrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(2, 2, 16)\n",
    "model2 = pytorchRNN(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 20000\n",
    "error = 0\n",
    "error2 = 0\n",
    "\n",
    "all_error = []\n",
    "all_loss = []\n",
    "\n",
    "dataset = BinaryDataset(8)\n",
    "\n",
    "for epoch in range(epoch_size):\n",
    "    x, y = next(dataset)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model2.zero_grad()\n",
    "    \n",
    "    output = model.forward(x)\n",
    "    output2 = model2.forward(x)\n",
    "    \n",
    "    loss = [output[i].crossentropy(y[i]) for i in range(len(y))]\n",
    "    loss2 = [torch.nn.CrossEntropyLoss(reduction='sum')(output2[i], torch.tensor(y[i].data[0,]).long()) for i in range(len(y))]\n",
    "    for l in loss[::-1]:\n",
    "        l.backward(np.array(1))\n",
    "    for l in loss2[::-1]:\n",
    "        l.backward(retain_graph=True)\n",
    "        \n",
    "    model.step(1e-2)\n",
    "    model2.step(1e-2)\n",
    "    \n",
    "    e = np.count_nonzero([np.all(output[i].argsoftmax().data != y[i].data) for i in range(len(y))])\n",
    "    error += e\n",
    "    all_error.append(e)\n",
    "    all_loss.append(sum([l.data for l in loss]))\n",
    "    \n",
    "    error2 += sum([(torch.max(output2[i], 1)[1] != torch.tensor(y[i].data[0,]).long()).sum().item() for i in range(len(y))])\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print('[{:5d}] error : {} - {}, loss : {} - {}'.format(epoch+1, error / 1000, error2 / 1000, sum([l.data for l in loss]), float(sum(loss2))))\n",
    "        error = 0\n",
    "        error2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_size=100\n",
    "\n",
    "tmp = all_error[:10000]\n",
    "\n",
    "x = [i*mean_size for i in range(len(tmp)//mean_size)]\n",
    "y = [sum(all_error[i*mean_size:(i+1)*mean_size])/ mean_size for i in range(len(tmp)//mean_size)]\n",
    "y2 = [sum(all_loss[i*mean_size:(i+1)*mean_size])/ mean_size for i in range(len(all_loss[:10000])//mean_size)]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Episode Error')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(x, y, '-o', label='error')\n",
    "plt.plot(x, y2, '-o', label='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_size = 1000\n",
    "error = 0\n",
    "error2 = 0\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "eval_dataset = BinaryDataset(50)\n",
    "\n",
    "for i in range(eval_size):\n",
    "    x, y = next(eval_dataset)\n",
    "    \n",
    "    output, _ = model.forward(x)\n",
    "    output2, _ = model2.forward(x)\n",
    "    \n",
    "    e = np.count_nonzero([np.all(output[i].argsoftmax().data != y[i].data) for i in range(len(y))])\n",
    "    error += e\n",
    "    error2 += sum([(torch.max(output2[i], 1)[1] != torch.tensor(y[i].data[0,]).long()).sum().item() for i in range(len(y))])\n",
    "    \n",
    "    \n",
    "    if e == 0:\n",
    "        accuracy += 1\n",
    "    \n",
    "    output = [float(o.argsoftmax().data) for o in output]\n",
    "    output2 = [float(torch.max(o, 1)[1]) for o in output2]\n",
    "    \n",
    "    x = np.concatenate([v.data for v in x]).T\n",
    "    y = np.concatenate([v.data for v in y]).T\n",
    "    \n",
    "    #print('[{:4d}] {:d} + {:d} = {:d}, m1:{}, m2:{}'.format(i, toNumber(x[0,:]), toNumber(x[1, :]), toNumber(y[0,:]), toNumber(output), toNumber(output2)))\n",
    "print('m1 error : {}, m2 error : {}'.format(error/eval_size, error2/eval_size))\n",
    "print('Accuracy : {:.3f}%'.format(accuracy / eval_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
