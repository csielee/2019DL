{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __save_model(model_name, model, root):\n",
    "    if not os.path.isdir(root):\n",
    "        os.mkdir(root)\n",
    "    p = os.path.join(root, '{}-params.pkl'.format(model_name))\n",
    "    torch.save(model.state_dict(), p)\n",
    "    return p\n",
    "\n",
    "def save_model(models, root='./model'):\n",
    "    p = {}\n",
    "    for k, m in models.items():\n",
    "        p[k] = __save_model(k, m, root)\n",
    "    return p\n",
    "\n",
    "def __load_model(model_name, model, root):\n",
    "    p = os.path.join(root, '{}-params.pkl'.format(model_name))\n",
    "    if not os.path.isfile(p):\n",
    "        raise AttributeError(\n",
    "            \"No model parameters file for {}!\".format(model_name)\n",
    "        )\n",
    "    paras = torch.load(p)\n",
    "    model.load_state_dict(paras)\n",
    "\n",
    "def load_model(models, root='./model'):\n",
    "    for k, m in models.items():\n",
    "        __load_model(k, m, root)\n",
    "        \n",
    "def save_model_by_score(models, loss, root):\n",
    "    p = os.path.join(root, 'state.pkl')\n",
    "    state = None\n",
    "    if os.path.isfile(p):\n",
    "         state = torch.load(p)\n",
    "            \n",
    "    if state is not None and state['loss'] < loss:\n",
    "        return;\n",
    "    \n",
    "    save_model(models, root)\n",
    "    state = {'loss' : loss}\n",
    "    torch.save(state, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDict:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "        \n",
    "        for i in range(26):\n",
    "            self.addWord(chr(ord('a') + i))\n",
    "        \n",
    "        tokens = [\"SOS\", \"EOS\"]\n",
    "        for t in tokens:\n",
    "            self.addWord(t)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "    def longtensorFromString(self, s):\n",
    "        s = [\"SOS\"] + list(s) + [\"EOS\"]\n",
    "        return torch.LongTensor([self.word2index[ch] for ch in s])\n",
    "    \n",
    "    def stringFromLongtensor(self, l):\n",
    "        s = \"\"\n",
    "        for i in l:\n",
    "            ch = self.index2word[i.item()]\n",
    "            if len(ch) > 1:\n",
    "                continue\n",
    "            s += ch\n",
    "        return s\n",
    "\n",
    "class wordsDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            f = './train.txt'\n",
    "        else:\n",
    "            f = './test.txt'\n",
    "        self.datas = np.loadtxt(f, dtype=np.str)\n",
    "        \n",
    "        if train:\n",
    "            self.datas = self.datas.reshape(-1)\n",
    "        else:\n",
    "            '''\n",
    "            sp -> p\n",
    "            sp -> pg\n",
    "            sp -> tp\n",
    "            sp -> tp\n",
    "            p  -> tp\n",
    "            sp -> pg\n",
    "            p  -> sp\n",
    "            pg -> sp\n",
    "            pg -> p\n",
    "            pg -> tp\n",
    "            '''\n",
    "            self.targets = np.array([\n",
    "                [0, 3],\n",
    "                [0, 2],\n",
    "                [0, 1],\n",
    "                [0, 1],\n",
    "                [3, 1],\n",
    "                [0, 2],\n",
    "                [3, 0],\n",
    "                [2, 0],\n",
    "                [2, 3],\n",
    "                [2, 1],\n",
    "            ])\n",
    "        \n",
    "        #self.tenses = ['sp', 'tp', 'pg', 'p']\n",
    "        self.tenses = [\n",
    "            'simple-present', \n",
    "            'third-person', \n",
    "            'present-progressive', \n",
    "            'simple-past'\n",
    "        ]\n",
    "        self.chardict = CharDict()\n",
    "        \n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            c = index % len(self.tenses)\n",
    "            return self.chardict.longtensorFromString(self.datas[index]), c\n",
    "        else:\n",
    "            i = self.chardict.longtensorFromString(self.datas[index, 0])\n",
    "            ci = self.targets[index, 0]\n",
    "            o = self.chardict.longtensorFromString(self.datas[index, 1])\n",
    "            co = self.targets[index, 1]\n",
    "            \n",
    "            return i, ci, o, co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, word_size, hidden_size, latent_size, \n",
    "        num_condition, condition_size\n",
    "    ):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.word_size = word_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.condition_size = condition_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.condition_embedding = nn.Embedding(num_condition, condition_size)\n",
    "        self.word_embedding = nn.Embedding(word_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.logvar = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, inputs, init_hidden, input_condition):\n",
    "        c = self.condition(input_condition)\n",
    "        \n",
    "        # get (1,1,hidden_size)\n",
    "        hidden = torch.cat((init_hidden, c), dim=2)\n",
    "        \n",
    "        # get (seq, 1, hidden_size)\n",
    "        x = self.word_embedding(inputs).view(-1, 1, self.hidden_size)\n",
    "        \n",
    "        # get (seq, 1, hidden_size), (1, 1, hidden_size)\n",
    "        outputs, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # get (1, 1, hidden_size)\n",
    "        m = self.mean(hidden)\n",
    "        logvar = self.logvar(hidden)\n",
    "        \n",
    "        z = self.sample_z() * torch.exp(logvar/2) + m\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(\n",
    "            1, 1, self.hidden_size - self.condition_size, \n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    def condition(self, c):\n",
    "        c = torch.LongTensor([c]).to(device)\n",
    "        return self.condition_embedding(c).view(1,1,-1)\n",
    "    \n",
    "    def sample_z(self):\n",
    "        return torch.normal(\n",
    "            torch.FloatTensor([0]*self.latent_size), \n",
    "            torch.FloatTensor([1]*self.latent_size)\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, word_size, hidden_size, latent_size, condition_size\n",
    "    ):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_size = word_size\n",
    "\n",
    "        self.latent_to_hidden = nn.Linear(\n",
    "            latent_size+condition_size, hidden_size\n",
    "        )\n",
    "        self.word_embedding = nn.Embedding(word_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, word_size)\n",
    "        \n",
    "    def forward(self, inputs, z, c):\n",
    "        # get (1,1,latent_size + condition_size)\n",
    "        latent = torch.cat((z, c), dim=2)\n",
    "        \n",
    "        # get (1,1,hidden_size)\n",
    "        hidden = self.latent_to_hidden(latent)\n",
    "        \n",
    "        # get (seq, 1, hidden_size)\n",
    "        x = self.word_embedding(inputs).view(-1, 1, self.hidden_size)\n",
    "        \n",
    "        # get (seq, 1, hidden_size), (1, 1, hidden_size)\n",
    "        outputs, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # get (seq, word_size)\n",
    "        outputs = self.out(outputs).view(-1, self.word_size)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "train_dataset = wordsDataset()\n",
    "test_dataset = wordsDataset(False)\n",
    "\n",
    "word_size = train_dataset.chardict.n_words\n",
    "num_condition = len(train_dataset.tenses)\n",
    "hidden_size = 256\n",
    "latent_size = 32\n",
    "condition_size = 8\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "empty_input_ratio = 0.1\n",
    "KLD_weight = 0.0\n",
    "LR = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN(\n",
       "   (condition_embedding): Embedding(4, 8)\n",
       "   (word_embedding): Embedding(28, 256)\n",
       "   (gru): GRU(256, 256)\n",
       "   (mean): Linear(in_features=256, out_features=32, bias=True)\n",
       "   (logvar): Linear(in_features=256, out_features=32, bias=True)\n",
       " ), DecoderRNN(\n",
       "   (latent_to_hidden): Linear(in_features=40, out_features=256, bias=True)\n",
       "   (word_embedding): Embedding(28, 256)\n",
       "   (gru): GRU(256, 256)\n",
       "   (out): Linear(in_features=256, out_features=28, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderRNN(\n",
    "    word_size, hidden_size, latent_size, num_condition, condition_size\n",
    ").to(device)\n",
    "decoder = DecoderRNN(\n",
    "    word_size, hidden_size, latent_size, condition_size\n",
    ").to(device)\n",
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO KL weights\n",
    "\n",
    "loss = cross_entropy + (kl w)*KL($q(Z|X, c;\\theta') || p(Z|c)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '{:4d}m {:2d}s'.format(int(m), int(s))\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def trainEpochs(\n",
    "    name, encoder, decoder, epoch_size, learning_rate=1e-2,\n",
    "    show_size=1000,\n",
    "):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    show_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    for epoch in range(epoch_size):\n",
    "        # get data from trian dataset\n",
    "        for idx in range(len(train_dataset)):   \n",
    "        #for idx in range(1):\n",
    "            data = train_dataset[idx]\n",
    "            inputs, c = data\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            # input no sos and eos\n",
    "            z = encoder(inputs[1:-1].to(device), encoder.initHidden(), c)\n",
    "            \n",
    "            # input has sos\n",
    "            outputs = decoder(inputs[:-1].to(device), z, encoder.condition(c))\n",
    "            \n",
    "            # target no sos\n",
    "            loss = criterion(outputs, inputs[1:].to(device))\n",
    "            #loss = criterion(outputs, inputs[:-1].to(device))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            \n",
    "            show_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "        \n",
    "        if (epoch + 1)%show_size == 0:\n",
    "            show_loss_total /= show_size\n",
    "            print(\"{} ({} {}%) {:.4f}\".format(\n",
    "                timeSince(start, (epoch+1) / epoch_size),\n",
    "                epoch+1, (epoch+1)*100/epoch_size, show_loss_total\n",
    "            ))\n",
    "            show_loss_total = 0\n",
    "            \n",
    "        plot_losses.append(plot_loss_total)\n",
    "        \n",
    "        save_model_by_score(\n",
    "            {'encoder':encoder, 'decoder':decoder}, \n",
    "            plot_loss_total, \n",
    "            os.path.join('.', name)\n",
    "        )\n",
    "        \n",
    "        plot_loss_total = 0\n",
    "        \n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0m  9s (-    1m 27s) (1 10.0%) 47180.8536\n",
      "   0m 19s (-    1m 16s) (2 20.0%) 27094.0264\n",
      "   0m 28s (-    1m  6s) (3 30.0%) 21986.9741\n",
      "   0m 38s (-    0m 57s) (4 40.0%) 19374.4651\n",
      "   0m 47s (-    0m 47s) (5 50.0%) 18304.5404\n",
      "   0m 56s (-    0m 37s) (6 60.0%) 18668.5839\n",
      "   1m  6s (-    0m 28s) (7 70.0%) 17745.1296\n",
      "   1m 15s (-    0m 18s) (8 80.0%) 18474.8561\n",
      "   1m 25s (-    0m  9s) (9 90.0%) 18910.2260\n",
      "   1m 34s (-    0m  0s) (10 100.0%) 19746.3051\n",
      "CPU times: user 1min 32s, sys: 1.77 s, total: 1min 34s\n",
      "Wall time: 1min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[47180.85358595848,\n",
       " 27094.026449918747,\n",
       " 21986.97412443161,\n",
       " 19374.4650554657,\n",
       " 18304.54042696953,\n",
       " 18668.583859443665,\n",
       " 17745.12957930565,\n",
       " 18474.856108903885,\n",
       " 18910.226037979126,\n",
       " 19746.3050968647]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainEpochs('result', encoder, decoder, 10, show_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute BLEU-4 score\n",
    "def compute_bleu(output, reference):\n",
    "    cc = SmoothingFunction()\n",
    "    return sentence_bleu(\n",
    "        [reference], output,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=cc.method1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
