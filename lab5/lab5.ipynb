{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __save_model(model_name, model, root):\n",
    "    if not os.path.isdir(root):\n",
    "        os.mkdir(root)\n",
    "    p = os.path.join(root, '{}-params.pkl'.format(model_name))\n",
    "    torch.save(model.state_dict(), p)\n",
    "    return p\n",
    "\n",
    "def save_model(models, root='./model'):\n",
    "    p = {}\n",
    "    for k, m in models.items():\n",
    "        p[k] = __save_model(k, m, root)\n",
    "    return p\n",
    "\n",
    "def __load_model(model_name, model, root):\n",
    "    p = os.path.join(root, '{}-params.pkl'.format(model_name))\n",
    "    if not os.path.isfile(p):\n",
    "        msg = \"No model parameters file for {}!\".format(model_name)\n",
    "        return print(msg)\n",
    "        raise AttributeError(msg)\n",
    "    paras = torch.load(p)\n",
    "    model.load_state_dict(paras)\n",
    "\n",
    "def load_model(models, root='./model'):\n",
    "    for k, m in models.items():\n",
    "        __load_model(k, m, root)\n",
    "        \n",
    "def save_model_by_score(models, loss, root):\n",
    "    p = os.path.join(root, 'state.pkl')\n",
    "    state = None\n",
    "    \n",
    "    if np.isnan(loss):\n",
    "        raise AttributeError(\"Loss become {}\".format(loss))\n",
    "        return\n",
    "    \n",
    "    if os.path.isfile(p):\n",
    "         state = torch.load(p)\n",
    "            \n",
    "    if state is not None and state['loss'] < loss:\n",
    "        return;\n",
    "    \n",
    "    save_model(models, root)\n",
    "    state = {'loss' : loss}\n",
    "    torch.save(state, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDict:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "        \n",
    "        for i in range(26):\n",
    "            self.addWord(chr(ord('a') + i))\n",
    "        \n",
    "        tokens = [\"SOS\", \"EOS\"]\n",
    "        for t in tokens:\n",
    "            self.addWord(t)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "    def longtensorFromString(self, s):\n",
    "        s = [\"SOS\"] + list(s) + [\"EOS\"]\n",
    "        return torch.LongTensor([self.word2index[ch] for ch in s])\n",
    "    \n",
    "    def stringFromLongtensor(self, l, show_token=False, check_end=True):\n",
    "        s = \"\"\n",
    "        for i in l:\n",
    "            ch = self.index2word[i.item()]\n",
    "            if len(ch) > 1:\n",
    "                if show_token:\n",
    "                    __ch = \"<{}>\".format(ch)\n",
    "                else:\n",
    "                    __ch = \"\"\n",
    "            else:\n",
    "                __ch = ch\n",
    "            s += __ch\n",
    "            if check_end and ch == \"EOS\":\n",
    "                break\n",
    "        return s\n",
    "\n",
    "class wordsDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            f = './train.txt'\n",
    "        else:\n",
    "            f = './test.txt'\n",
    "        self.datas = np.loadtxt(f, dtype=np.str)\n",
    "        \n",
    "        if train:\n",
    "            self.datas = self.datas.reshape(-1)\n",
    "        else:\n",
    "            '''\n",
    "            sp -> p\n",
    "            sp -> pg\n",
    "            sp -> tp\n",
    "            sp -> tp\n",
    "            p  -> tp\n",
    "            sp -> pg\n",
    "            p  -> sp\n",
    "            pg -> sp\n",
    "            pg -> p\n",
    "            pg -> tp\n",
    "            '''\n",
    "            self.targets = np.array([\n",
    "                [0, 3],\n",
    "                [0, 2],\n",
    "                [0, 1],\n",
    "                [0, 1],\n",
    "                [3, 1],\n",
    "                [0, 2],\n",
    "                [3, 0],\n",
    "                [2, 0],\n",
    "                [2, 3],\n",
    "                [2, 1],\n",
    "            ])\n",
    "        \n",
    "        #self.tenses = ['sp', 'tp', 'pg', 'p']\n",
    "        self.tenses = [\n",
    "            'simple-present', \n",
    "            'third-person', \n",
    "            'present-progressive', \n",
    "            'simple-past'\n",
    "        ]\n",
    "        self.chardict = CharDict()\n",
    "        \n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            c = index % len(self.tenses)\n",
    "            return self.chardict.longtensorFromString(self.datas[index]), c\n",
    "        else:\n",
    "            i = self.chardict.longtensorFromString(self.datas[index, 0])\n",
    "            ci = self.targets[index, 0]\n",
    "            o = self.chardict.longtensorFromString(self.datas[index, 1])\n",
    "            co = self.targets[index, 1]\n",
    "            \n",
    "            return i, ci, o, co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, word_size, hidden_size, latent_size, \n",
    "        num_condition, condition_size\n",
    "    ):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.word_size = word_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.condition_size = condition_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.condition_embedding = nn.Embedding(num_condition, condition_size)\n",
    "        self.word_embedding = nn.Embedding(word_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.logvar = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, inputs, init_hidden, input_condition):\n",
    "        c = self.condition(input_condition)\n",
    "        \n",
    "        # get (1,1,hidden_size)\n",
    "        hidden = torch.cat((init_hidden, c), dim=2)\n",
    "        \n",
    "        # get (seq, 1, hidden_size)\n",
    "        x = self.word_embedding(inputs).view(-1, 1, self.hidden_size)\n",
    "        \n",
    "        # get (seq, 1, hidden_size), (1, 1, hidden_size)\n",
    "        outputs, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # get (1, 1, hidden_size)\n",
    "        m = self.mean(hidden)\n",
    "        logvar = self.logvar(hidden)\n",
    "        \n",
    "        z = self.sample_z() * torch.exp(logvar/2) + m\n",
    "        \n",
    "        #self.m = m\n",
    "        #self.logvar = logvar\n",
    "        \n",
    "        return z, m, logvar\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(\n",
    "            1, 1, self.hidden_size - self.condition_size, \n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    def condition(self, c):\n",
    "        c = torch.LongTensor([c]).to(device)\n",
    "        return self.condition_embedding(c).view(1,1,-1)\n",
    "    \n",
    "    def sample_z(self):\n",
    "        return torch.normal(\n",
    "            torch.FloatTensor([0]*self.latent_size), \n",
    "            torch.FloatTensor([1]*self.latent_size)\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, word_size, hidden_size, latent_size, condition_size\n",
    "    ):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_size = word_size\n",
    "\n",
    "        self.latent_to_hidden = nn.Linear(\n",
    "            latent_size+condition_size, hidden_size\n",
    "        )\n",
    "        self.word_embedding = nn.Embedding(word_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, word_size)\n",
    "        \n",
    "    def forward(self, inputs, z, c, teacher=False, hidden=None):\n",
    "        # get (1,1,latent_size + condition_size)\n",
    "        latent = torch.cat((z, c), dim=2)\n",
    "        \n",
    "        # get (1,1,hidden_size)\n",
    "        if hidden is None:\n",
    "            hidden = self.latent_to_hidden(latent)\n",
    "            #print(\"get hidden from latent\")\n",
    "        \n",
    "        # get (seq, 1, hidden_size)\n",
    "        x = self.word_embedding(inputs).view(-1, 1, self.hidden_size)\n",
    "        \n",
    "        input_length = x.size(0)\n",
    "        \n",
    "        # get (seq, 1, hidden_size), (1, 1, hidden_size)\n",
    "        if teacher:\n",
    "            outputs = []\n",
    "            for i in range(input_length-1):\n",
    "                output, hidden = self.gru(x[i:i+1], hidden)\n",
    "                hidden = x[i+1:i+2]\n",
    "                outputs.append(output)\n",
    "            \n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            # Omit EOS token\n",
    "            x = x[:-1]\n",
    "            outputs, hidden = self.gru(x, hidden)\n",
    "            \n",
    "        # get (seq, word_size)\n",
    "        outputs = self.out(outputs).view(-1, self.word_size)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "train_dataset = wordsDataset()\n",
    "test_dataset = wordsDataset(False)\n",
    "\n",
    "word_size = train_dataset.chardict.n_words\n",
    "num_condition = len(train_dataset.tenses)\n",
    "hidden_size = 256\n",
    "latent_size = 32\n",
    "condition_size = 8\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "empty_input_ratio = 0.1\n",
    "KLD_weight = 0.0\n",
    "LR = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN(\n",
       "   (condition_embedding): Embedding(4, 8)\n",
       "   (word_embedding): Embedding(28, 256)\n",
       "   (gru): GRU(256, 256)\n",
       "   (mean): Linear(in_features=256, out_features=32, bias=True)\n",
       "   (logvar): Linear(in_features=256, out_features=32, bias=True)\n",
       " ), DecoderRNN(\n",
       "   (latent_to_hidden): Linear(in_features=40, out_features=256, bias=True)\n",
       "   (word_embedding): Embedding(28, 256)\n",
       "   (gru): GRU(256, 256)\n",
       "   (out): Linear(in_features=256, out_features=28, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderRNN(\n",
    "    word_size, hidden_size, latent_size, num_condition, condition_size\n",
    ").to(device)\n",
    "decoder = DecoderRNN(\n",
    "    word_size, hidden_size, latent_size, condition_size\n",
    ").to(device)\n",
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(\n",
    "    {'encoder':encoder, 'decoder':decoder}, \n",
    "    os.path.join('.', 'temp')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO KL weights\n",
    "\n",
    "loss = cross_entropy + (kl w)*KL($q(Z|X, c;\\theta') || p(Z|c)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLD_weight_annealing(*args):\n",
    "    epoch, batch = args\n",
    "    slope = 0.001\n",
    "    #slope = 0.1\n",
    "    scope = (1.0 / slope)*2\n",
    "    \n",
    "    w = (epoch % scope) * slope\n",
    "    \n",
    "    if w > 1.0:\n",
    "        w = 1.0\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '{:4d}m {:2d}s'.format(int(m), int(s))\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def KL_loss(m, logvar):\n",
    "    return torch.sum(0.5 * (-logvar + (m**2) + torch.exp(logvar) - 1))\n",
    "\n",
    "def trainEpochs(\n",
    "    name, encoder, decoder, epoch_size, learning_rate=1e-2,\n",
    "    show_size=1000, KLD_weight=0.0, teacher_forcing_ratio = 1.0\n",
    "):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    show_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    plot_kl_loss_total = 0\n",
    "    char_accuracy_total = 0\n",
    "    char_accuracy_len = 0\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        # get data from trian dataset\n",
    "        for idx in range(len(train_dataset)):   \n",
    "        #for idx in range(1):\n",
    "            data = train_dataset[idx]\n",
    "            inputs, c = data\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            # input no sos and eos\n",
    "            z, m, logvar = encoder(inputs[1:-1].to(device), encoder.initHidden(), c)\n",
    "            \n",
    "            # decide teacher forcing\n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            \n",
    "            # input has sos\n",
    "            outputs, _ = decoder(inputs.to(device), z, encoder.condition(c), use_teacher_forcing)\n",
    "            \n",
    "            # target no sos\n",
    "            loss = criterion(outputs, inputs[1:].to(device))\n",
    "            kld_loss = KL_loss(m, logvar)\n",
    "            #loss = criterion(outputs, inputs[:-1].to(device))\n",
    "            if callable(KLD_weight):\n",
    "                kld_w = KLD_weight(epoch, idx)\n",
    "            else:\n",
    "                kld_w = KLD_weight\n",
    "                \n",
    "            #print('crossentropy : {} , kld : {}'.format(loss.item(), kld_loss.item()))\n",
    "                \n",
    "            (loss + (kld_w * kld_loss)).backward()\n",
    "            \n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            show_loss_total += loss.item() + ( kld_w*kld_loss.item() )\n",
    "            plot_loss_total += loss.item()\n",
    "            plot_kl_loss_total += kld_loss.item()\n",
    "            \n",
    "            # show output by string\n",
    "            # outputs_onehot = torch.max(outputs, 1)[1]\n",
    "            outputs_onehot = torch.max(torch.softmax(outputs, dim=1), 1)[1]\n",
    "            inputs_str = train_dataset.chardict.stringFromLongtensor(inputs, show_token=True)\n",
    "            outputs_str = train_dataset.chardict.stringFromLongtensor(outputs_onehot, show_token=True)\n",
    "            #print(inputs_str,':',outputs_str)\n",
    "            \n",
    "            char_accuracy_total += (outputs_onehot[:-1] == inputs[1:-1].to(device)).sum().item()\n",
    "            char_accuracy_len += len(inputs[1:-1])\n",
    "        \n",
    "        if (epoch + 1)%show_size == 0:\n",
    "            show_loss_total /= show_size\n",
    "            print(\"{} ({} {}%) {:.4f}\".format(\n",
    "                timeSince(start, (epoch+1) / epoch_size),\n",
    "                epoch+1, (epoch+1)*100/epoch_size, show_loss_total\n",
    "            ))\n",
    "            print('accuracy : {}'.format(char_accuracy_total/char_accuracy_len))\n",
    "            show_loss_total = 0\n",
    "            \n",
    "        plot_losses.append((plot_loss_total, plot_kl_loss_total))\n",
    "        \n",
    "        if callable(KLD_weight):\n",
    "            kld_w = KLD_weight(epoch, idx)\n",
    "        else:\n",
    "            kld_w = KLD_weight\n",
    "        \n",
    "        #print(epoch, kld_w)\n",
    "        \n",
    "        save_model_by_score(\n",
    "            {'encoder':encoder, 'decoder':decoder}, \n",
    "            plot_loss_total + kld_w * plot_kl_loss_total, \n",
    "            os.path.join('.', name)\n",
    "        )\n",
    "        \n",
    "        plot_loss_total = 0\n",
    "        plot_kl_loss_total = 0\n",
    "        char_accuracy_total = 0\n",
    "        char_accuracy_len = 0\n",
    "        \n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainEpochs('result', encoder, decoder, 500, show_size=10, KLD_weight=KLD_weight_annealing, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder': './best/encoder-params.pkl',\n",
       " 'decoder': './best/decoder-params.pkl'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model({'encoder':encoder, 'decoder':decoder}, \n",
    "    os.path.join('.', 'best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute BLEU-4 score\n",
    "def compute_bleu(output, reference):\n",
    "    cc = SmoothingFunction()\n",
    "    return sentence_bleu(\n",
    "        [reference], output,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=cc.method1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(encoder, decoder, dataset):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    char_accuracy_total = 0\n",
    "    char_accuracy_len = 0\n",
    "    \n",
    "    blue_score = []\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        data = dataset[idx]\n",
    "        if dataset.train:\n",
    "            inputs, input_condition = data\n",
    "            targets = inputs\n",
    "            target_condition = input_condition\n",
    "        else:\n",
    "            inputs, input_condition, targets, target_condition = data\n",
    "            \n",
    "        # input no sos and eos\n",
    "        z, _, _ = encoder(inputs[1:-1].to(device), encoder.initHidden(), input_condition)\n",
    "            \n",
    "        # input has sos\n",
    "        outputs, _ = decoder(targets.to(device), z, encoder.condition(target_condition), False)\n",
    "            \n",
    "        # show output by string\n",
    "        outputs_onehot = torch.max(torch.softmax(outputs, dim=1), 1)[1]\n",
    "        targets_str = train_dataset.chardict.stringFromLongtensor(targets, check_end=True)\n",
    "        outputs_str = train_dataset.chardict.stringFromLongtensor(outputs_onehot, check_end=True)\n",
    "        \n",
    "        #print(targets_str,':',outputs_str)\n",
    "            \n",
    "        char_accuracy_total += (outputs_onehot[:-1] == targets[1:-1].to(device)).sum().item()\n",
    "        char_accuracy_len += len(targets[1:-1])\n",
    "        \n",
    "        blue_score.append( compute_bleu(outputs_str, targets_str) )\n",
    "    \n",
    "    print('Accuracy per char : {}'.format(char_accuracy_total / char_accuracy_len))\n",
    "    print('BLEU-4 score : {}'.format(sum(blue_score) / len(blue_score)))\n",
    "    \n",
    "    return blue_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per char : 0.8205898268398268\n",
      "BLEU-4 score : 0.5458485626650489\n"
     ]
    }
   ],
   "source": [
    "all_score = evaluation(encoder, decoder, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(encoder, decoder, z, condition, maxlen=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    z = z.view(1,1,-1)\n",
    "    sos_token = train_dataset.chardict.word2index['SOS']\n",
    "    eos_token = train_dataset.chardict.word2index['EOS']\n",
    "    inputs = torch.LongTensor([sos_token, eos_token])\n",
    "    outputs = []\n",
    "    i = 0\n",
    "    hidden = None\n",
    "    \n",
    "    while True:\n",
    "        # get (1, word_size)\n",
    "        output, hidden = decoder(\n",
    "            inputs.to(device), \n",
    "            z.to(device), \n",
    "            encoder.condition(condition),\n",
    "            False,\n",
    "            hidden\n",
    "        )\n",
    "        output_onehot = torch.max(torch.softmax(output, dim=1), 1)[1]\n",
    "        if output_onehot.item() == eos_token:\n",
    "            break\n",
    "        \n",
    "        outputs.append(output_onehot.item())\n",
    "        i += 1\n",
    "        if maxlen <= i:\n",
    "            break\n",
    "        \n",
    "        inputs = torch.LongTensor([outputs[-1], eos_token])\n",
    "        \n",
    "    return torch.LongTensor(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = encoder.sample_z()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3527,  0.1130,  0.5260, -0.5226,  0.2138, -1.8048, -0.4402,  0.9119,\n",
      "         0.5118, -1.7090, -0.7055,  0.5366,  0.5434,  0.9837, -0.0902, -1.7488,\n",
      "        -0.6951, -0.2378, -1.1977, -0.2207,  0.7286, -1.0850, -0.5777,  0.3871,\n",
      "        -0.8316,  1.4539, -0.3098, -1.2312,  0.2934,  1.2168, -0.9046,  0.0234],\n",
      "       device='cuda:0')\n",
      "simple-present       : blooze\n",
      "third-person         : blooks\n",
      "present-progressive  : blooming\n",
      "simple-past          : bloomed\n"
     ]
    }
   ],
   "source": [
    "print(noise)\n",
    "for i in range(len(train_dataset.tenses)):\n",
    "    outputs = generate_word(encoder, decoder, noise, i)\n",
    "    output_str = train_dataset.chardict.stringFromLongtensor(outputs)\n",
    "    print('{:20s} : {}'.format(train_dataset.tenses[i],output_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2851,  1.2386,  1.0263,  1.1914, -1.4874, -1.2694,  0.6261, -0.4220,\n",
      "        -0.6354,  0.1487, -2.7937, -1.4543,  0.1237,  0.7397, -1.0753,  0.2940,\n",
      "         0.0773, -0.9082,  1.0397, -0.0071,  0.0673, -0.0568, -0.3548,  0.2373,\n",
      "        -0.2261, -0.8434,  0.6118,  0.2808,  1.0806, -0.1785,  0.0830, -0.6146],\n",
      "       device='cuda:0')\n",
      "simple-present       : throw\n",
      "third-person         : throws\n",
      "present-progressive  : throwing\n",
      "simple-past          : throwed\n"
     ]
    }
   ],
   "source": [
    "print(noise)\n",
    "for i in range(len(train_dataset.tenses)):\n",
    "    outputs = generate_word(encoder, decoder, noise, i)\n",
    "    output_str = train_dataset.chardict.stringFromLongtensor(outputs)\n",
    "    print('{:20s} : {}'.format(train_dataset.tenses[i],output_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
