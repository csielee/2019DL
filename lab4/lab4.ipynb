{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable():\n",
    "    def __init__(self, data, T=None, grad=None, copy=True):\n",
    "        if data is None or type(data) != np.ndarray:\n",
    "            raise AttributeError('Wrong data type')\n",
    "        \n",
    "        if copy:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            self.data = data\n",
    "        if grad is None:\n",
    "            grad = np.zeros_like(self.data)\n",
    "        self.grad = grad\n",
    "        if T is None:\n",
    "            T = Variable(self.data.T, self, self.grad.T, copy=False)\n",
    "        self.T = T\n",
    "        self.fn = None\n",
    "        self.child = []\n",
    "        self.ready = False\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad[:,:] = 0.0\n",
    "        self.child = []\n",
    "        self.ready = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Variable(\\n{}\\n)\\n'.format(self.data.__str__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.data.__str__()\n",
    "    \n",
    "    def __add__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "            \n",
    "        c = Variable(self.data + b.data)\n",
    "        c.fn = [Variable.__grad_add__, self, b]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_add__(self, a, b):\n",
    "        a.grad += np.ones_like(a.grad) * self.grad\n",
    "        b.grad += np.ones_like(b.grad) * self.grad\n",
    "    \n",
    "    def __sub__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "        c = Variable(self.data - b.data)\n",
    "        c.fn = [Variable.__grad_sub__, self, b]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_sub__(self, a, b):\n",
    "        a.grad += np.ones_like(a.grad) * self.grad\n",
    "        b.grad -= np.ones_like(b.grad) * self.grad\n",
    "    \n",
    "    def __mul__(self, b):\n",
    "        if type(b) is not Variable:\n",
    "            b = Variable(np.ones_like(self.data)*b)\n",
    "        \n",
    "        c = Variable(self.data * b.data)\n",
    "        c.fn = [Variable.__grad_mul__, self, b]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_mul__(self, a, b):\n",
    "        a.grad += b.data * self.grad\n",
    "        b.grad += a.data * self.grad\n",
    "    \n",
    "    def __matmul__(self, b):\n",
    "        c = Variable(np.matmul(self.data, b.data))\n",
    "        c.fn = [Variable.__grad_matmul__, self, b]\n",
    "           \n",
    "        self.child.append(c)\n",
    "        b.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_matmul__(self, a, b):\n",
    "        a.grad += np.matmul(self.grad, b.data.T)\n",
    "        b.grad += np.matmul(a.data.T, self.grad)\n",
    "    \n",
    "    \n",
    "    def tanh(self):\n",
    "        c = Variable(np.tanh(self.data))\n",
    "        c.fn = [Variable.__grad_tanh__, self]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        return c\n",
    "        \n",
    "    def __grad_tanh__(self, a):\n",
    "        a.grad += self.grad * (1 - (self.data**2))\n",
    "    \n",
    "    def crossentropy(self, target):\n",
    "        s = self.softmax(1)\n",
    "        if type(target) is Variable:\n",
    "            target = target.data\n",
    "            \n",
    "        target = target.astype(np.int)\n",
    "        \n",
    "        if target.shape[0] > 1:\n",
    "            slis = tuple(zip(range(target.shape[0]), target))\n",
    "        else:\n",
    "            slis = (0, target[0])\n",
    "        \n",
    "        c = Variable(np.array(np.sum(-np.log(s[slis]))))\n",
    "        c.fn = [Variable.__grad_corssentropy, self, target]\n",
    "        \n",
    "        self.child.append(c)\n",
    "        return c\n",
    "    \n",
    "    def __grad_corssentropy(self, a, target):\n",
    "        y = np.zeros_like(a.grad)\n",
    "        if target.shape[0] > 1:\n",
    "            slis = tuple(zip(range(target.shape[0]), target))\n",
    "        else:\n",
    "            slis = (0, target[0])\n",
    "            \n",
    "        y[slis] = 1.0\n",
    "        a.grad += (a.softmax(1) - y)\n",
    "    \n",
    "    def softmax(self, dim):\n",
    "        # move dim idxs\n",
    "        exp_data = np.exp(self.data)\n",
    "        return exp_data / np.sum(exp_data, axis=dim).reshape([-1]+[1 for _ in range(dim)])\n",
    "    \n",
    "    def argsoftmax(self):\n",
    "        s = self.softmax(1)\n",
    "        return Variable(np.argmax(s, axis=1).reshape(-1,1))\n",
    "    \n",
    "    def backward(self, backward_grad):\n",
    "        if type(backward_grad) is Variable:\n",
    "            backward_grad = backward_grad.data\n",
    "        \n",
    "        if backward_grad.shape != self.data.shape:\n",
    "            raise AttributeError('Wrong backward grad shape {} != {}'.format(backward_grad.shape, self.data.shape))\n",
    "        \n",
    "        self.grad = backward_grad\n",
    "        self.__backward()\n",
    "    \n",
    "    def __backward(self):\n",
    "        if self.fn is None:\n",
    "            return;\n",
    "        \n",
    "        # check self grad is ready, trace child variables\n",
    "        self.ready = True\n",
    "        for child in self.child:\n",
    "            self.ready &= child.ready\n",
    "        \n",
    "        if not self.ready:\n",
    "            return;\n",
    "        \n",
    "        backward_op = self.fn[0]\n",
    "        \n",
    "        backward_op(self, *self.fn[1:])\n",
    "        \n",
    "        for v in self.fn[1:]:\n",
    "            if type(v) is Variable:\n",
    "                v.__backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6278228097737124 tensor(1.6278, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def testGrad():\n",
    "    n = 1\n",
    "    c = 5\n",
    "    x = np.random.random((n,2))\n",
    "    h = np.random.random((n,16))\n",
    "    y = np.random.randint(0, c, n)\n",
    "    \n",
    "    u = np.random.random((2, 16))\n",
    "    w = np.random.random((16,16))\n",
    "    v = np.random.random((16, c))\n",
    "    \n",
    "    def equation(*args):\n",
    "        x, h, u, w, v = args\n",
    "        h1 = ((x @ u) + (h @ w)).tanh()\n",
    "        return ((x @ u) + (h1 @ w)).tanh() @ v\n",
    "    \n",
    "    def allv(l):\n",
    "        return [Variable(i) for i in l]\n",
    "\n",
    "    def allt(l):\n",
    "        return [torch.tensor(i, requires_grad=True) for i in l]\n",
    "    \n",
    "    wow = [x, h, u, w, v]\n",
    "    \n",
    "    Vs = allv(wow)\n",
    "    Ts = allt(wow)\n",
    "    Y = Variable(y)\n",
    "    _Y = torch.tensor(y.astype(np.float), requires_grad=True)\n",
    "    \n",
    "    A = equation(*Vs)\n",
    "    A = A.crossentropy(Y)\n",
    "    _A = equation(*Ts)\n",
    "    _A = torch.nn.CrossEntropyLoss(reduction='sum')(_A, _Y.long())\n",
    "    #_A = torch.nn.MSELoss()(_A, _A)\n",
    "    \n",
    "    print(A, _A)\n",
    "    \n",
    "    A.backward(np.array(1))\n",
    "    _A.backward()\n",
    "    \n",
    "    t = None\n",
    "    \n",
    "    for V, T in zip(Vs, Ts):\n",
    "        r = torch.allclose(torch.tensor(V.grad), T.grad)\n",
    "        print(torch.allclose(torch.tensor(V.grad), T.grad))\n",
    "        if t is None:\n",
    "            t = r\n",
    "        else:\n",
    "            t &= r\n",
    "    \n",
    "    return t\n",
    "testGrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels):\n",
    "        self.U = Variable(np.random.uniform(-1,1, (in_channels, hidden_channels)))\n",
    "        self.W = Variable(np.random.uniform(-1,1, (hidden_channels, hidden_channels)))\n",
    "        self.V = Variable(np.random.uniform(-1,1, (hidden_channels, out_channels)))\n",
    "        self.b = Variable(np.random.uniform(-1,1, (1, hidden_channels)))\n",
    "        self.c = Variable(np.random.uniform(-1,1, (1, out_channels)))\n",
    "        self.h = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        t = len(x)\n",
    "        self.h = None\n",
    "        y = []\n",
    "        hs = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            a = self.b + (x[i] @ self.U)\n",
    "            if self.h is not None:\n",
    "                a += (self.h @ self.W)\n",
    "        \n",
    "            self.h = a.tanh()\n",
    "            hs.append(self.h)\n",
    "            \n",
    "            o = self.c + (self.h @ self.V)\n",
    "            y.append(o)\n",
    "        \n",
    "        return y, hs\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.U.zero_grad()\n",
    "        self.W.zero_grad()\n",
    "        self.V.zero_grad()\n",
    "        self.b.zero_grad()\n",
    "        self.c.zero_grad()\n",
    "    \n",
    "    def step(self, lr=1e-1):\n",
    "        self.U.data -= lr * self.U.grad\n",
    "        self.W.data -= lr * self.W.grad\n",
    "        self.V.data -= lr * self.V.grad\n",
    "        self.b.data -= lr * self.b.grad\n",
    "        self.c.data -= lr * self.c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorchRNN():\n",
    "    def __init__(self, model):\n",
    "        self.U = torch.tensor(model.U.data, requires_grad=True)\n",
    "        self.W = torch.tensor(model.W.data, requires_grad=True)\n",
    "        self.V = torch.tensor(model.V.data, requires_grad=True)\n",
    "        self.b = torch.tensor(model.b.data, requires_grad=True)\n",
    "        self.c = torch.tensor(model.c.data, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        t = len(x)\n",
    "        self.h = None\n",
    "        y = []\n",
    "        hs = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            a = self.b + (torch.tensor(x[i].data, requires_grad=True) @ self.U)\n",
    "            if self.h is not None:\n",
    "                a += (self.h @ self.W)\n",
    "            self.h = a.tanh()\n",
    "            hs.append(self.h)\n",
    "            \n",
    "            o = self.c + (self.h @ self.V)\n",
    "            y.append(o)\n",
    "            \n",
    "        return y, hs\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        if self.U.grad is None:\n",
    "            return;\n",
    "        with torch.no_grad():\n",
    "            self.U.grad.zero_()\n",
    "            if self.W.grad is not None:\n",
    "                self.W.grad.zero_()\n",
    "            self.V.grad.zero_()\n",
    "            self.b.grad.zero_()\n",
    "            self.c.grad.zero_()\n",
    "    \n",
    "    def step(self, lr=1e-1):\n",
    "        if self.U.grad is None:\n",
    "            return;\n",
    "        with torch.no_grad():\n",
    "            self.U -= lr * self.U.grad\n",
    "            if self.W.grad is not None:\n",
    "                self.W -= lr * self.W.grad\n",
    "            self.V -= lr * self.V.grad\n",
    "            self.b -= lr * self.b.grad\n",
    "            self.c -= lr * self.c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toBinaray(x, digits, complement=False):\n",
    "    if complement and x < 0:\n",
    "        x += (1<<(digits))\n",
    "    x = abs(x)\n",
    "    return [ float(int(i)) for i in list((\"{:0\" + str(digits) + \"b}\").format(x))[::-1]][:digits]\n",
    "\n",
    "def toNumber(b, complement=False):\n",
    "    if complement:\n",
    "        last = b[-1]\n",
    "        b = b[:-1]\n",
    "    d = sum([int(x)<<i for i, x in enumerate(b)])\n",
    "    if complement:\n",
    "        d -= int(last)*(1<<(len(b)))\n",
    "    return d\n",
    "\n",
    "def BinaryDataset(digits=8):\n",
    "    thr = (1<<(digits-1))\n",
    "    while True:\n",
    "        a, b = np.random.randint(0, thr, 2)\n",
    "        c = a + b\n",
    "        x = np.array([toBinaray(a, digits), toBinaray(b, digits)])\n",
    "        y = np.array([toBinaray(c, digits)])\n",
    "        yield [Variable(x.T[i:i+1, :]) for i in range(digits)], [Variable(y.T[i:i+1, :]) for i in range(digits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(2, 2, 16)\n",
    "model2 = pytorchRNN(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1000] error : 3.464 - 3.47, loss : 6.787096388418801 - 6.4107264009101605\n",
      "[ 2000] error : 2.275 - 2.18, loss : 6.112918221100947 - 7.724017438327864\n",
      "[ 3000] error : 1.062 - 0.878, loss : 0.3218426310338222 - 0.3245673247588545\n",
      "[ 4000] error : 0.062 - 0.054, loss : 0.08897429517948031 - 0.0706128636280523\n",
      "[ 5000] error : 0.0 - 0.0, loss : 0.040026431988435196 - 0.04500407209151014\n",
      "[ 6000] error : 0.0 - 0.0, loss : 0.02236197762822727 - 0.04026761795416345\n",
      "[ 7000] error : 0.0 - 0.0, loss : 0.015575632767256702 - 0.01658517600234699\n",
      "[ 8000] error : 0.0 - 0.0, loss : 0.03005009823441702 - 0.02003097368407758\n",
      "[ 9000] error : 0.0 - 0.0, loss : 0.021729447287012562 - 0.024331664871079628\n",
      "[10000] error : 0.0 - 0.0, loss : 0.006876849113934807 - 0.014882849798210529\n",
      "[11000] error : 0.0 - 0.0, loss : 0.012847021414003275 - 0.009033295059531099\n",
      "[12000] error : 0.0 - 0.0, loss : 0.010358946705204052 - 0.014153454555267508\n",
      "[13000] error : 0.0 - 0.0, loss : 0.01685293693257689 - 0.012228518763244267\n",
      "[14000] error : 0.0 - 0.0, loss : 0.007504827534360923 - 0.009966122147201606\n",
      "[15000] error : 0.0 - 0.0, loss : 0.006381775317761776 - 0.006615895051612863\n",
      "[16000] error : 0.0 - 0.0, loss : 0.01220752869502386 - 0.0052306167766627\n",
      "[17000] error : 0.0 - 0.0, loss : 0.003900054442822834 - 0.004876973051472611\n",
      "[18000] error : 0.0 - 0.0, loss : 0.006940571878024875 - 0.006380321471331385\n",
      "[19000] error : 0.0 - 0.0, loss : 0.012895054981908806 - 0.01840322861555288\n",
      "[20000] error : 0.0 - 0.0, loss : 0.026363117198432744 - 0.03202938925352106\n"
     ]
    }
   ],
   "source": [
    "epoch_size = 20000\n",
    "error = 0\n",
    "error2 = 0\n",
    "\n",
    "dataset = BinaryDataset(8)\n",
    "\n",
    "for epoch in range(epoch_size):\n",
    "    x, y = next(dataset)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model2.zero_grad()\n",
    "    \n",
    "    output, hs = model.forward(x)\n",
    "    output2, hs2 = model2.forward(x)\n",
    "    \n",
    "    loss = [output[i].crossentropy(y[i]) for i in range(len(y))]\n",
    "    loss2 = [torch.nn.CrossEntropyLoss(reduction='sum')(output2[i], torch.tensor(y[i].data[0,]).long()) for i in range(len(y))]\n",
    "    for l in loss[::-1]:\n",
    "        l.backward(np.array(1))\n",
    "    for l in loss2[::-1]:\n",
    "        l.backward(retain_graph=True)\n",
    "        \n",
    "    model.step(1e-2)\n",
    "    model2.step(1e-2)\n",
    "    \n",
    "    error += np.count_nonzero([np.all(output[i].argsoftmax().data != y[i].data) for i in range(len(y))])\n",
    "    error2 += sum([(torch.max(output2[i], 1)[1] != torch.tensor(y[i].data[0,]).long()).sum().item() for i in range(len(y))])\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print('[{:5d}] error : {} - {}, loss : {} - {}'.format(epoch+1, error / 1000, error2 / 1000, sum([l.data for l in loss]), float(sum(loss2))))\n",
    "        error = 0\n",
    "        error2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 error : 0.0, m2 error : 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_size = 1000\n",
    "error = 0\n",
    "error2 = 0\n",
    "\n",
    "eval_dataset = BinaryDataset(50)\n",
    "\n",
    "for i in range(eval_size):\n",
    "    x, y = next(eval_dataset)\n",
    "    \n",
    "    output, _ = model.forward(x)\n",
    "    output2, _ = model2.forward(x)\n",
    "    \n",
    "    error += np.count_nonzero([np.all(output[i].argsoftmax().data != y[i].data) for i in range(len(y))])\n",
    "    error2 += sum([(torch.max(output2[i], 1)[1] != torch.tensor(y[i].data[0,]).long()).sum().item() for i in range(len(y))])\n",
    "    \n",
    "    output = [float(o.argsoftmax().data) for o in output]\n",
    "    output2 = [float(torch.max(o, 1)[1]) for o in output2]\n",
    "    \n",
    "    x = np.concatenate([v.data for v in x]).T\n",
    "    y = np.concatenate([v.data for v in y]).T\n",
    "    \n",
    "    #print('[{:4d}] {:d} + {:d} = {:d}, m1:{}, m2:{}'.format(i, toNumber(x[0,:]), toNumber(x[1, :]), toNumber(y[0,:]), toNumber(output), toNumber(output2)))\n",
    "print('m1 error : {}, m2 error : {}'.format(error/eval_size, error2/eval_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
